{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show casing MLP vs CNN\n",
    "\n",
    "\n",
    "![](img/MlpvsCnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras model module\n",
    "\n",
    "    # Multilayer Perceptron is a sequence of multiple neural network Layers\n",
    "    # Import the Sequential model type from Keras. This is an interface to build a linear stack of neural network layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras core layers\n",
    "    \n",
    "    Import the \"core\" layers from Keras. These are the layers that are used in almost any neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras CNN layers\n",
    "\n",
    "     Import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import convolutional, pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load image data from MNIST.\n",
    "\n",
    "    MNIST database of handwritten digits\n",
    "\n",
    "    Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first few images of train\n",
    "from matplotlib import pyplot\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first few images of train\n",
    "from matplotlib import pyplot\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(X_test[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns:\n",
    "\n",
    "    2 tuples:\n",
    "            x_train, x_test: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n",
    "            y_train, y_test: uint8 array of digit labels (integers in range 0-9) with shape (num_samples,).\n",
    "\n",
    "Arguments:\n",
    "\n",
    "    path: if you do not have the index file locally (at '~/.keras/datasets/' + path), it will be downloaded to this location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for MNIST Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MNIST Data for MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our data type to float32 and normalize our data values to the range [0, 1]\n",
    "\n",
    "Note: Max value X_train/X_test can take is 255, so it is divided by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess class labels for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape of our class label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have 10 different classes, one for each digit, but it looks like we only have a 1-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print (y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Convert 1-dimensional class arrays to 10-dimensional class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same number of samples, but flatten the 28, 28 to 28*28 (784)\n",
    "\n",
    "X_train_mlp = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "\n",
    "X_test_mlp = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train_mlp.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_mlp, Y_train, validation_split = 0.2,\n",
    "                    batch_size = 48000, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_mlp, Y_test, verbose=0)\n",
    "\n",
    "print(\"[INFO] test score - {}\".format(scores[0]))\n",
    "\n",
    "print(\"[INFO] test accuracy - {}\".format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "digit = model.predict_classes(X_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digit[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\dhanasree\\\\Desktop\\\\JNTUA_NN\\\\CNN-MNIST\\\\sample_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict a new image\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "    # load the image\n",
    "    img = load_img(filename, grayscale=True, target_size=(28, 28))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 1 channel\n",
    "    img = img.reshape(1,784)\n",
    "    # prepare pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image and predict the class\n",
    "def run_example():\n",
    "    # load the image\n",
    "    img = load_image('sample_image.png')\n",
    "    # load model\n",
    "    model = load_model('final_model.h5')\n",
    "    # predict the class\n",
    "    digit = model.predict_classes(img)\n",
    "    print(digit[0])\n",
    "#entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will see on CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are CNNs?\n",
    "\n",
    "<br />\n",
    "\n",
    "* How are images represented again?\n",
    "\n",
    "<br />\n",
    "\n",
    "![](img/image_representation.gif)\n",
    "\n",
    "<br />\n",
    "\n",
    "* What is a CNN?\n",
    "\n",
    "![](img/lecun_net.png)\n",
    "\n",
    "<br />\n",
    "\n",
    "* The four main operations in a CNN:\n",
    "\n",
    "    * Convolution\n",
    "    * Non Linearity (ReLU)\n",
    "    * Pooling or Sub Sampling\n",
    "    * Classification (Fully Connected Layer)\n",
    "\n",
    "<br />\n",
    "\n",
    "* Some important convolving operations used in ML based image classifiers, before deep learning\n",
    "\n",
    "<br />\n",
    "\n",
    "![](img/prior_convolving_operations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/convolutions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/convolution_matrix.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/convolution_real_image.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Feature Map Dimensions after Convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/conv_output_feature_map_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        W - Width of the input feature map\n",
    "        \n",
    "        K - Width of the Kernel / Filter\n",
    "        \n",
    "        P - Padding Layer dimension\n",
    "        \n",
    "        S - Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Padding in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  In Keras we have two types of Padding:\n",
    "    \n",
    "      (i) Valid Padding : Any layer that does not fully intersect with the filter is ignored (NO PADDING!)\n",
    "        \n",
    "      (ii) Same Padding : Adds Zero Padding such that the output feature map has similar dimensions to the input                      feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/zero_padding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/same_padding.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for MNIST Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess input data for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape input data\n",
    "\n",
    "    When using the tensorflow backend, you must explicitly declare a dimension for the depth of the input image. \n",
    "    E.g. a full-color image with all 3 RGB channels will have a depth of 3.\n",
    "\n",
    "    Our MNIST images only have a depth of 1, but we must explicitly declare that. \n",
    "\n",
    "    In other words, we want to transform our dataset from having shape (n, width, height) to (n, width, height, channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " print X_train's dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (X_train_cnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the input layer\n",
    "\n",
    "CNN input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1,1), activation='relu', input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### 2D convolution layer (e.g. spatial convolution over images).\n",
    "\n",
    "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n",
    "\n",
    "When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(28, 28, 1) for 28x28 gray pictures in data_format=\"channels_last\".\n",
    "\n",
    "First parameters correspond to the number of convolution filters \n",
    "\n",
    "Next 2 parameters correspond to kernal size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Add more layers to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/max_pooling.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(filters = 32, kernel_size = (3, 3), strides = (1,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(filters = 64, kernel_size = (3, 3), strides = (1,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(filters = 32, kernel_size = (3, 3), strides = (1,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout\n",
    "\n",
    "    This is a method for regularizing our model in order to prevent overfitting. \n",
    "\n",
    "MaxPooling2D \n",
    "\n",
    "    Is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
    "\n",
    "So far, for model parameters, we've added two Convolution layers. To complete our model architecture, let's add a fully connected layer and then the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n",
    "\n",
    "Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n",
    "\n",
    "Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model by providing the loss function and the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_adam = Adam(lr = 0.01, decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss = 'categorical_crossentropy', optimizer = custom_adam, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_model.fit(X_train_cnn, Y_train, batch_size = 10000, epochs = 2, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(X_test_cnn, Y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = cnn_model.predict(X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = cnn_model.predict_classes(X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict new image\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "    # load the image\n",
    "    img = load_image('sample_image.png')\n",
    "    # load model\n",
    "    model = load_model('final_model2.h5')\n",
    "    # predict the class\n",
    "    digit = model.predict_classes(img)\n",
    "    print(digit[0])\n",
    "#entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "\n",
    "    https://keras.io/\n",
    "    https://elitedatascience.com\n",
    "    http://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I AM STILL LEARNING.........my thirst for learning is an unfilled vessel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
